{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b497b92-f759-467e-bda5-3d6fe597fa42",
   "metadata": {},
   "source": [
    "# Using 'open virtual dataset' capability to work with TEMPO Level 3 data\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we will use the `earthaccess.open_virtual_mfdataset()` function to open a week's worth of granules from the Nitrogen Dioxide (NO<sub>2</sub>) Level-3 data collection of the [TEMPO air quality mission](https://asdc.larc.nasa.gov/project/TEMPO).\n",
    "\n",
    "**About TEMPO**: The Tropospheric Emissions: Monitoring of Pollution (TEMPO) instrument is a geostationary satellite mission that provides hourly daytime measurements of air quality over North America. It measures key pollutants including nitrogen dioxide (NO<sub>2</sub>), formaldehyde, and ozone at high spatial resolution (~2 by 4.75 km at the center of its field of regard).\n",
    "\n",
    "We will calculate temporal and spatial means for a subset of the data and visualize the results. This approach demonstrates cloud-optimized data access patterns that can scale from days to years of data.\n",
    "\n",
    "**Learn more**: For comprehensive documentation on the `earthaccess` package, visit the [earthaccess documentation](https://earthaccess.readthedocs.io/).\n",
    "\n",
    "Note that this same approach can be used for a date range of any length, within the mission's duration. Running this notebook for a year's worth of TEMPO Level-3 data took approximately 15 minutes.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **AWS US-West-2 Environment:** This tutorial has been designed to run in an AWS cloud compute instance in AWS region us-west-2. However, if you want to run it from your laptop or workstation, everything should work just fine but without the speed benefits of in-cloud access.\n",
    "\n",
    "- **Earthdata Account:** A (free!) Earthdata Login account is required to access data from the NASA Earthdata system. Before requesting TEMPO data, we first need to set up our Earthdata Login authentication, as described in the Earthdata Cookbook's [earthaccess tutorial (link)](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/tutorials/earthaccess-demo.html).\n",
    "\n",
    "- **Packages:**\n",
    "\n",
    "  - `cartopy`\n",
    "  - `dask`\n",
    "  - `earthaccess` version 0.14.0 or greater\n",
    "  - `matplotlib`\n",
    "  - `numpy`\n",
    "  - `xarray`\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a11af-f182-4021-ad6f-8b198d6bdbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import earthaccess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import rcParams\n",
    "\n",
    "%config InlineBackend.figure_format = 'jpeg'\n",
    "rcParams[\"figure.dpi\"] = (\n",
    "    80  # Reduce figure resolution to keep the saved size of this notebook low.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5f134f-f9d0-4427-9561-b5e7604748d9",
   "metadata": {},
   "source": [
    "## Login using the Earthdata Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198318d6-2850-450e-9df0-dd0895ee985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = earthaccess.login()\n",
    "\n",
    "if not auth.authenticated:\n",
    "    # Ask for credentials and persist them in a .netrc file.\n",
    "    auth.login(strategy=\"interactive\", persist=True)\n",
    "\n",
    "print(earthaccess.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3e22a-3af7-4584-9517-7993a8bad9c0",
   "metadata": {},
   "source": [
    "## Search for data granules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70e220-893c-4451-bfaa-c01e14e0b577",
   "metadata": {},
   "source": [
    "We search for TEMPO Nitrogen Dioxide (NO<sub>2</sub>) data for a week-long period (note: times are in UTC), between January 11th and 18th, 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50079f-6b8b-424d-8a12-36416f4d69ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    # TEMPO NO₂ Level-3 product\n",
    "    short_name=\"TEMPO_NO2_L3\",\n",
    "    # Version 3 of the data product\n",
    "    version=\"V03\",\n",
    "    # Time period: One week in January 2024 (times are in UTC)\n",
    "    temporal=(\"2024-01-11 12:00\", \"2024-01-18 12:00\"),\n",
    ")\n",
    "\n",
    "print(f\"Number of granules found: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c752f56-dfd4-43d0-a786-28446d95ddb0",
   "metadata": {},
   "source": [
    "## Opening Virtual Multifile Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347119e30bdc19c6",
   "metadata": {},
   "source": [
    "## Understanding Virtual Datasets\n",
    "\n",
    "Virtual datasets allow us to work with multiple files as if they were a single dataset without downloading all the data to local storage. This is achieved through:\n",
    "\n",
    "1. **Kerchunk**: Creates lightweight reference files that point to data chunks in cloud storage\n",
    "2. **Virtualizarr**: Combines multiple reference files into a single virtual dataset\n",
    "3. **Lazy Loading**: Data is only accessed when needed for computations\n",
    "\n",
    "For TEMPO data, we need to handle the hierarchical netCDF4 structure by opening each group separately, then merging them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca9042-11f1-42da-88eb-39086739555a",
   "metadata": {},
   "source": [
    "First we set the argument options to be used by `earthaccess.open_virtual_mfdataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba056eb9cc6c95",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "`load` argument considerations:\n",
    "\n",
    "- `load=True` works.  Within `earthaccess.open_virtual_mfdataset`, a temporary virtual reference file (a \"virtual dataset\") is created and then immediately loaded with kerchunk. This is because the function assumes the user is making this request for the first time and the combined manifest file needs to be generated first. In the future, however, `earthaccess.open_virtual_mfdataset` may provide a way to save the combined manifest file, at which point you could then avoid repeating these steps, and proceed directly to loading with kerchunk/virtualizarr.\n",
    "- `load=False` results in `KeyError: \"no index found for coordinate 'longitude'\"` because it creates `ManifestArray`s without indexes (see the [earthaccess documentation here (link)](https://github.com/nsidc/earthaccess/blob/7f5fe5d2e42343b6d7948338255cf9bb8cdb2775/earthaccess/dmrpp_zarr.py#L36C456-L36C502))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f96bd-c21a-4563-87d8-76110e1ffbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_options = {\n",
    "    \"access\": \"direct\",  # Direct access to cloud data (faster in AWS)\n",
    "    \"load\": True,  # Load metadata immediately (required for indexing)\n",
    "    \"concat_dim\": \"time\",  # Concatenate files along the time dimension\n",
    "    \"data_vars\": \"minimal\",  # Only load data variables that include the concat_dim\n",
    "    \"coords\": \"minimal\",  # Only load coordinate variables that include the concat_dim\n",
    "    \"compat\": \"override\",  # Avoid coordinate conflicts by picking the first\n",
    "    \"combine_attrs\": \"override\",  # Avoid attribute conflicts by picking the first\n",
    "    \"parallel\": True,  # Enable parallel processing with Dask\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9cb42-5b76-4a63-a9b2-52ffb3c9fd8a",
   "metadata": {},
   "source": [
    "Because TEMPO data are processed and archived in a netCDF4 format using a group hierarchy,\n",
    "we open each group – i.e., 'root', 'product', and 'geolocation' – and then afterwards merge them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defaf2cb-5efd-44aa-b6c8-a53ce4990aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result_root = earthaccess.open_virtual_mfdataset(granules=results, **open_options)\n",
    "result_product = earthaccess.open_virtual_mfdataset(\n",
    "    granules=results, group=\"product\", **open_options\n",
    ")\n",
    "result_geolocation = earthaccess.open_virtual_mfdataset(\n",
    "    granules=results, group=\"geolocation\", **open_options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39b6d6a-0ad0-444b-bea0-734f41a1c838",
   "metadata": {},
   "source": [
    "Merge root groups with subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a4b23-ac38-4622-9823-1b42ddb6a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_merged = xr.merge([result_root, result_product, result_geolocation])\n",
    "result_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ff2a521b9705b",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "\n",
    "- **vertical_column_troposphere**: Total column amount of NO₂ in the troposphere (units: molecules/cm²)\n",
    "- **main_data_quality_flag**: Quality indicator (0 = good quality data)\n",
    "- **Geographic region**: We'll focus on the Mid-Atlantic region (Washington DC area)\n",
    "  - Longitude: -78° to -74° W\n",
    "  - Latitude: 35° to 39° N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20295784e3556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our region of interest (Mid-Atlantic/Washington DC area)\n",
    "lon_bounds = (-78, -74)  # Western to Eastern longitude\n",
    "lat_bounds = (35, 39)  # Southern to Northern latitude\n",
    "\n",
    "print(\n",
    "    f\"Analyzing region: {lat_bounds[0]}°N to {lat_bounds[1]}°N, {abs(lon_bounds[0])}°W to {abs(lon_bounds[1])}°W\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adafb1-3362-4b5e-8e7a-9171b49368e9",
   "metadata": {},
   "source": "## Temporal Mean - a map showing an annual average"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a8a83-412a-4552-a5a6-616974140410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define temporal mean (average over time) calculation\n",
    "temporal_mean_ds = (\n",
    "    result_merged.sel(\n",
    "        {\n",
    "            \"longitude\": slice(lon_bounds[0], lon_bounds[1]),\n",
    "            \"latitude\": slice(lat_bounds[0], lat_bounds[1]),\n",
    "        }\n",
    "    )\n",
    "    .where(result_merged[\"main_data_quality_flag\"] == 0)  # Filter for good quality data\n",
    "    .mean(dim=\"time\")\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape after subsetting: {temporal_mean_ds.dims}\")\n",
    "temporal_mean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a507704-b598-4b61-87af-cae5c324364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the temporal mean\n",
    "mean_vertical_column_trop = temporal_mean_ds[\"vertical_column_troposphere\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0733bc1-c3d9-4c2a-bbf6-0c9942cce75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "mean_vertical_column_trop.squeeze().plot.contourf(ax=ax)\n",
    "\n",
    "# Add geographic features\n",
    "ax.coastlines()\n",
    "ax.gridlines(\n",
    "    draw_labels=True,\n",
    "    dms=True,\n",
    "    x_inline=False,\n",
    "    y_inline=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c386e7-dcc1-43ca-bbe2-a81169e240ee",
   "metadata": {},
   "source": [
    "## Spatial mean - a time series of area averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf504b-d6f2-48e5-98ee-72d4645a129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define spatial mean (average over longitude/latitude) calculation\n",
    "spatial_mean_ds = (\n",
    "    result_merged.sel(\n",
    "        {\n",
    "            \"longitude\": slice(lon_bounds[0], lon_bounds[1]),\n",
    "            \"latitude\": slice(lat_bounds[0], lat_bounds[1]),\n",
    "        }\n",
    "    )\n",
    "    .where(result_merged[\"main_data_quality_flag\"] == 0)  # Filter for good quality data\n",
    "    .mean(dim=(\"longitude\", \"latitude\"))\n",
    ")\n",
    "spatial_mean_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e68d63-beb0-4dbe-97a7-182e4c338b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the spatial mean\n",
    "spatial_mean_vertical_column_trop = spatial_mean_ds[\n",
    "    \"vertical_column_troposphere\"\n",
    "].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10b7c5-e9ba-42be-94eb-faec55dfa667",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_mean_vertical_column_trop.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49910b-546f-4f47-a91d-8f6bee0aebf4",
   "metadata": {},
   "source": [
    "## Single scan subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4279f0-b837-4744-940e-98cc51be5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single scan time for detailed analysis\n",
    "scan_time_start = np.datetime64(\"2024-01-11T13:00:00\")  # 1 PM UTC\n",
    "scan_time_end = np.datetime64(\"2024-01-11T14:00:00\")  # 2 PM UTC\n",
    "\n",
    "print(f\"Analyzing single scan: {scan_time_start} to {scan_time_end} UTC\")\n",
    "print(\"Note: This corresponds to ~8-9 AM local time on the US East Coast\")\n",
    "\n",
    "subset_ds = result_merged.sel(\n",
    "    {\n",
    "        \"longitude\": slice(lon_bounds[0], lon_bounds[1]),\n",
    "        \"latitude\": slice(lat_bounds[0], lat_bounds[1]),\n",
    "        \"time\": slice(scan_time_start, scan_time_end),\n",
    "    }\n",
    ").where(result_merged[\"main_data_quality_flag\"] == 0)\n",
    "subset_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35dfc9-b019-42cf-8a2e-17e50da0ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the single scan's values\n",
    "subset_vertical_column_trop = subset_ds[\"vertical_column_troposphere\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3660d3-c2a9-41fe-8b96-3f13ee143801",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "subset_vertical_column_trop.squeeze().plot.contourf(ax=ax)\n",
    "\n",
    "# Add geographic features\n",
    "ax.coastlines()\n",
    "ax.gridlines(\n",
    "    draw_labels=True,\n",
    "    dms=True,\n",
    "    x_inline=False,\n",
    "    y_inline=False,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6b034-6b49-4787-b3d3-ab80b2198a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
